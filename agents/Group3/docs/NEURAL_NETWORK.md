# Neural Network
This document describes the neural network used to guide the Monte Carlo Tree Search, including its input representation, architecture, training procedure, and integration into the search process. The network is designed to provide efficient and informative evaluations that improve search quality under limited simulation budgets.

## Input representation
The neural network operates on a representation of the Hex board encoded as a two-dimensional NumPy array. Each cell in the array represents one of three possible states: empty, occupied by the current player, or occupied by the opponent. This representation captures the full game state at each move and is passed to the network whenever a leaf node requires evaluation during MCTS.

To ensure consistency between players, board canonicalisation is applied so that the network always evaluates positions from a fixed perspective, allowing a single network to be used for both Red and Blue players.

## Network Architecture
The model is based on an Azalea-style convolutional neural network architecture designed for Hex. It consists of shared convolutional layers followed by two output heads: a policy head and a value head. The policy head outputs a probability distribution over all possible moves, while the value head outputs a scalar estimate of the expected game outcome from the current position. This dual-head design allows the network to both guide exploration and evaluate leaf states.

## Training
The network was trained using self-play data generated by the MCTS agent. Training was initialised from a pretrained Azalea baseline to provide a strong prior and accelerate convergence. Self-play games were used to generate training examples consisting of board states, improved move distributions obtained from MCTS visit counts, and final game outcomes.

Due to computational constraints, the amount of self-play data was limited; however, starting from a pretrained model allowed the network to achieve reasonable performance with relatively few training iterations.

## Batching and inference
During both training and evaluation, neural network inference is performed in batches to reduce overhead and improve computational efficiency. Batching is particularly important during MCTS, where multiple leaf nodes may require evaluation in quick succession. By grouping evaluations together, the system reduces the frequency of network calls and improves overall simulation throughput.

## Integration with MCTS
When the C++ MCTS engine encounters a leaf node requiring evaluation, the corresponding board state is passed to Python via the interface layer. The neural network processes the input and returns a policy distribution and value estimate. These outputs are then incorporated into the search tree: policy priors initialise child nodes during expansion, and the value estimate is used during backup to update node statistics.

This tight integration allows the neural network to influence both exploration and evaluation while keeping the core search loop efficient.
